\documentclass[11pt]{article}

\usepackage{fullpage,hyperref,hypcap,namedplus,amsmath,enumitem}
\hypersetup{colorlinks=true,allcolors=black}

\title{\bf Deconvoluting Dropout Events in Single Cell RNA-Seq Count Data}
\author{\bf Guilherme de Sena Brandine}
\date{\today}

\begin{document}

\maketitle

\section{Motivation}
Singe-cell RNA-Seq is a technique that allows transcriptome profiles of tissues in a single-cell resolution. It has a wide variety of applications that range from analysis of tissue heterogeneity in complex systems like tumors and blood tissues to temporal reconstruction of differentiation processes like embryogenesis. However, due to the small amount of starting material that is the set of messenger RNAs in a single cell, data generated by this procedure suffers from large amounts of technical noise. One particular obstacle in analyzing such data are \textbf{dropout events}, in which the messenger RNA fails to be converted into cDNA, and as such it is not amplified and in all posterior steps of analysis the gene's count will be set to zero, whereas in truth it is actually expressed. Since many genes in a single cell are not expressed for other biological reasons (eg, different stages of cell cycle or genes that simply aren't relevant to the cell's speciality), it is difficult to decide the actual origin of a zero. \\
\\
Here we propose a method that observes the cell's nearest neighbors (eg, cells with similar expression profile as measured by Euclidean distance) and treats their counts for a particular gene as samplings for a random variable modeled as the product of a Negative Binomial (as usually done for RNA-Seq counts) and a Bernoulli trial whose success probability is dependent on the cell's true expression value (eg, the higher the expression, the lower the dropout probability), and uses Maximum Likelihood Estimation of this modeled random variable to infer the most probable value of the gene's expression average. 

\section{Method}
The method described below makes the following assumptions:
\\
1) Cells that have a similar expression profile will have smaller Euclidean distance and will remain similar even after dropouts (as we expect roughly the same genes to be dropped out in both cells). \\
\\
2) If we ignore the existance of dropouts, the reads behave like in bulk cell RNA-Seq following a negative-binomial distribution as proposed by DESeq and edgeR. \cite{anders2010differential} \cite{kharchenko2014bayesian}\\
\\
3) The dropout rate varies with the mean expression value through the following relationship: The probability of a dropout to occur is equal to $p = e^{-\lambda \mu}$, where $\lambda$ is an experiment-specific constant and $\mu$ is the expression value for a gene in a cell. \cite{pierson2015zifa}\\
\\
let $Y_{ij}$ be the read of gene $i$ in cell $j$. We model $Y_{ij}$ as:

\begin{equation}
\begin{split}
Y_{ij} \sim B_{ij} W_{ij} \\
B_{ij} \sim Bernoulli (1 - \exp (-\lambda \mu_{ij})) \\
W_{ij} \sim NB(\mu_{ij}, \mu_{ij} + \alpha \mu_{ij}^{2})
\end{split}
\end{equation}

Here we assume parameters $\lambda$ and $\alpha$ are known. $\lambda$ is a constant that models the dropout frequency and $\alpha$ models the negative binomial dispersion in a cell (technical noise). Both values are experiment-specific and can be inferred from the data. \\
\\
Our goal here is to estimate $\mu_{ij}$, the true expression value of the cell's read. We use reads from the cell's $k$ nearest neighbors - as well as its own - for the same gene $i$ to make a pseudo set of samplings for the cell: $\vec{y_i} = \{y_{i,1}, \dots, y_{i,k+1} \}$. We then define the log-likelihood for the parameter $\mu_{ij}$ as:
$$
L(\mu | \vec{y_i}) = \sum_{j=1}^{k+1} \log P(Y_{ij} = y_{i,j}) 
$$

(For simplicity, we wrote - and will henceforth write - $\mu_{ij}$ as simply $\mu$)\\
To infer the formula for $P(Y_{ij} = y_{i,j})$, we use the total probability formula conditioning on whether or not the dropout $B_{ij}$ happened:

\begin{equation} \label{tpf}
P(Y_{ij} = y_{i,j}) = P(Y_{ij} = y_{i,j} | B_{ij} = 0) P(B_{ij} = 0) + P(Y_{ij} = y_{i,j} | B_{ij} = 1)P (B_{ij} = 1)
\end{equation}

Before we proceed, we recall some properties of the negative binomial distribution. if $X \sim NB (r, p)$ we have:

\begin{equation} \label{probs}
P(X = x) = \frac{\Gamma(r+x-1)}{\Gamma(r-1) \Gamma(x)} p^x (1-p)^r
\end{equation}

Its expected value and variance are, respectively, $\mu = \frac{pr}{1-p}$ and variance is $\sigma^2 = \frac{pr}{(1-p)^2}$. From these equations, we can isolate $p$ and $r$ as a function of $\mu$ and $\sigma^2$ to conclude that: $p = \frac{\mu}{\sigma^2}$ and $r = \frac{\mu^2}{\sigma^2 - \mu}$. When modeling the cell's read counts as a negative binomial as in (\ref{probs}), we assume the reads are dispersed around the true read value, and thus the mean is a good estimate of the true read. Thus, instead of parametrizing the negative binomial by the values $r$ and $p$, we use a single parameter value $\mu$ assuming the variance and mean are correlated by the relationship $\sigma^2 = \mu + \alpha \mu^2$, where $\alpha$ is known, whence we have: $p = \frac{1}{1+ \alpha\mu}$ and $r = \frac{1}{\alpha}$. \\
\\
Plugging this on (\ref{probs}) we arrive at the following expression for $P(Y_{ij} = y_{i,j})$:

$$
 P(Y_{ij} = y_{i,j}) = e^{-\lambda \mu}I_{\{y_{i,j} = 0\}} + (1 - e^{-\lambda \mu}) \frac{\Gamma (y_{i,j} + 1/\alpha - 1)}{\Gamma (y_{i,j}) \Gamma (1/\alpha - 1)} \left(\frac{1}{1+\alpha\mu}\right)^{\frac{1}{\alpha}} \left(1 - \frac{1}{1+ \alpha\mu}\right)^{y_{i,j}}
$$

Here, $I_{\{y_{i,j} = 0\}}$ is the indicator function which is 1 if $y_{i,j} = 0$ and 0 otherwise, meaning the first term of equation \ref{tpf} will only be nonzero if $y_{i,j} = 0$ (eg, a nonzero read is impossible when a dropout has happened). We thus can separate the log-likelihood in zero and nonzero values. \\
\\
Setting $U(y) = \frac{\Gamma(y+1/\alpha-1)}{\Gamma(y)\Gamma(1/\alpha - 1)}$ The nonzero values become:

$$
\log P(Y_{ij} = y_{i,j}) = \log{(1 - e^{-\lambda\mu})} + \log {U(y_{i,j})} - \left( \frac{1}{\alpha} +y_{i,j} \right) \log {(1+\alpha \mu)} - y_{i,j} \log {\alpha \mu}
$$

Noting that $U(0) = 1$, the zero values become:
$$
\log P(Y_{ij} = 0) = \log \left[ e^{-\lambda \mu} + (1 - e^{-\lambda \mu}) \left(\frac{1}{1+\alpha\mu}\right)^{\frac{1}{\alpha}} \right]
$$

Intuitively, this value sums the probability of observing a zero both by a dropout event and by random chance. The MLE of $\mu$ is the value that solves $\frac{d L(\mu | \vec{y_i})}{d \mu} = 0$.\\ 
\\
The derivative of the log probabilities of the nonzero values is:
$$
\frac{d}{d \mu} \log P(Y_{ij} = y_{i,j}) = \frac{\lambda e^{-\lambda \mu}}{1 - e^{-\lambda \mu}} - \frac{1 + \alpha y_{i,j}}{1 + \alpha \mu} - \frac{\alpha y_{i,j}}{\mu}
$$
\\
And the derivative of the zero log probabilities is:
$$
\frac{d}{d \mu} \log P(Y_{ij} = 0) = \frac{\lambda e^{-\lambda \mu}\left( \left( \frac{1}{1+\alpha \mu}\right)^{1/\alpha} -1 \right) + (1 - e^{\lambda \mu}) \alpha \left( \frac{1}{1+\alpha\mu} \right)^{1/\alpha + 1}}{e^{-\lambda \mu} + (1 - e^{-\lambda \mu}) \left(\frac{1}{1+\alpha\mu}\right)^{\frac{1}{\alpha}}}
$$

Suppose that among our $k+1$ observations of the expression value we have $z$ zeros and $k+1-z$ nonzero values. Also let $S_i = \sum_{j=1}^{k+1} y_{i,j}$, then our value of $\mu$ is finally the root of the following equation:
$$
(k+1-z) \left( \frac{\lambda e^{-\lambda\mu}}{1 - e^{-\lambda \mu}} + \frac{1}{1+\alpha\mu}\right) + S_i\alpha\left( \frac{1}{1+\alpha\mu} - \frac{1}{\mu}\right) + z \frac{\lambda e^{-\lambda \mu}\left( \left( \frac{1}{1+\alpha \mu}\right)^{1/\alpha} -1 \right) + (1 - e^{\lambda \mu}) \alpha \left( \frac{1}{1+\alpha\mu} \right)^{1/\alpha + 1}}{e^{-\lambda \mu} + (1 - e^{-\lambda \mu}) \left(\frac{1}{1+\alpha\mu}\right)^{\frac{1}{\alpha}}} = 0
$$
\\
\\
Interestingly, only one term in this equation depends on the nonzero data, which is $S_i\alpha\left( \frac{1}{1+\alpha\mu} - \frac{1}{\mu}\right)$, and the third term is weighted by the number of nearest neighbors that are counted as zero .For the more general case, we need numerical methods to find the root of this equation, so questions arise: \\
\\
1) What is the most appropriate numerical algorithm to solve this equation given it has exponential and polynomial terms? (maybe plot the function to have an idea of how it behaves based on parameters $S$, $\lambda$ and $\alpha$?) \\
\\
2) How to estimate $\lambda$ and $\alpha$ from the data? (I think there are some papers, including DESeq, which proposes the mean-variance relationship in the negative binomial that go through this) \\
\\
3) Since we're only estimating one parameter, it is possible we only need a few nearest neighbors for a good estimate of $\mu$, but what exactly would be an ideal value of $k$? 

\section{Validation of the Method}

Validation would be performed in simulated data. Fixing $\alpha$ and $\lambda$, a reference cell would be created in which many base expression values would be zero and some would be nonzero. Expression values would follow a Poisson distribution with a fixed mean of 20. Afterwards, noisy cell reads would be simulated by sampling a negative binomial * bernoulli for each gene given the mean expression values. The method would be then applied and the true mean x estimated mean plot would be created, in which we'd expect points to fall along the 45 degree line. We could also look for public data in which we have an estimate of the expected library size for the cells and compare library size pre- and post- drop-out deconvolution. 
\section{References}

\bibliographystyle{plain}
\bibliography{mybib}


\end{document}
